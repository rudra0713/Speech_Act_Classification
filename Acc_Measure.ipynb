{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doomed  929089077084938241\n",
      "ground truth\n",
      "[2, 2, 0, 2, 0]\n",
      "3643\n",
      "[3, 2, 0, 2, 0]\n",
      "3642\n",
      "Detailed evaluation:\n",
      "precision: [0.84274711 0.         0.88859833 0.43846154 0.20588235 0.        ]\n",
      "recall: [0.89016949 0.         0.84865135 0.46341463 0.25       0.        ]\n",
      "fscore: [0.86580943 0.         0.86816556 0.45059289 0.22580645 0.        ]\n",
      "support: [1475   13 2002  123   28    1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "ground_truth = []\n",
    "with open('bert_data/new_data/data_30_sa/test_30_sa_backup.tsv') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    \n",
    "    for i,row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if int(row[2]) > 4:\n",
    "            print(\"doomed \", row[0])\n",
    "\n",
    " \n",
    "        ground_truth.append(int(row[2]))\n",
    "        \n",
    "print(\"ground truth\")\n",
    "print(ground_truth[:5])\n",
    "print(len(ground_truth))\n",
    "speech_act_labels = ['Assertion', 'Recommendation', 'Expression', 'Question', 'Request', 'Miscellaneous']\n",
    "sentiment_labels = ['Positive', 'Negative', 'Neutral', 'Mixed']\n",
    "\n",
    "\n",
    "df_results = pd.read_csv(\"bert_data/new_data/data_30_sa/test_results.tsv\",sep=\"\\t\",header=None)\n",
    "df_results_csv = pd.DataFrame({'Is_Response':df_results.idxmax(axis=1)})\n",
    "\n",
    "classification = []\n",
    "for row in df_results_csv.itertuples():\n",
    "    a,b = row\n",
    "    classification.append(b)\n",
    "print(classification[:5])\n",
    "print(len(classification))\n",
    "\n",
    "precision, recall, fscore, support = score(np.array(ground_truth[:-1]), classification)\n",
    "print(\"Detailed evaluation:\")\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "[3, 3, 2, 15, 4]\n",
      "21825\n",
      "[23, 1, 0, 15, 1]\n",
      "21825\n",
      "Detailed evaluation:\n",
      "precision: [0.         0.00013351 0.03846154 0.04032258 0.08426073 0.\n",
      " 0.00123001 0.0088645  0.19672131 0.         0.         0.\n",
      " 0.         0.00059488 0.         0.91469194 0.01470588 0.33333333\n",
      " 0.03571429 0.01023891 0.         0.         0.01333333 0.\n",
      " 0.         0.         0.         0.         0.05555556 0.\n",
      " 0.         0.         0.         0.00578035 0.5        0.\n",
      " 0.         0.         0.25806452 0.         0.         0.79310345\n",
      " 0.33333333 0.8        0.         0.         1.         0.\n",
      " 0.         0.        ]\n",
      "recall: [0.         0.00483092 0.00524476 0.0027248  0.00722957 0.\n",
      " 0.01769912 0.01895307 0.00315375 0.         0.         0.\n",
      " 0.         0.00980392 0.         1.         0.00063694 0.00692841\n",
      " 0.01785714 0.05084746 0.         0.         0.00940439 0.\n",
      " 0.         0.         0.         0.         0.4        0.\n",
      " 0.         0.         0.         0.11111111 0.00390625 0.\n",
      " 0.         0.         0.21621622 0.         0.         0.95833333\n",
      " 0.33333333 0.8        0.         0.         1.         0.\n",
      " 0.         0.        ]\n",
      "fscore: [0.         0.00025984 0.00923077 0.00510465 0.01331658 0.\n",
      " 0.00230017 0.01207938 0.00620797 0.         0.         0.\n",
      " 0.         0.0011217  0.         0.95544554 0.001221   0.01357466\n",
      " 0.02380952 0.01704545 0.         0.         0.01102941 0.\n",
      " 0.         0.         0.         0.         0.09756098 0.\n",
      " 0.         0.         0.         0.01098901 0.00775194 0.\n",
      " 0.         0.         0.23529412 0.         0.         0.86792453\n",
      " 0.33333333 0.8        0.         0.         1.         0.\n",
      " 0.         0.        ]\n",
      "support: [ 112  207  572 1835 7331   85  113 1108 3805 2506  133  139   75  102\n",
      "  285  193 1570  433  112   59   12   56  319   25    8   65   55   22\n",
      "    5   12   32    2    2    9  256    3   34   10   37    5   11   24\n",
      "    3   10   25    1    3    2    1    1]\n",
      "weighted f score  0.018015948392891883\n",
      "average f score  0.08849201124867055\n",
      "conf mat\n",
      "[[  0.   4.   0. ...   0.   0.   0.]\n",
      " [  3.   1.   0. ...   0.   0.   0.]\n",
      " [429.  27.   3. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   1. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, statistics\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "ground_truth = []\n",
    "with open('switchboard_data/test_switch_backup.tsv') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    for i,row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        ground_truth.append(int(row[2]))\n",
    "        \n",
    "print(\"ground truth\")\n",
    "print(ground_truth[:5])\n",
    "print(len(ground_truth))\n",
    "\n",
    "\n",
    "df_results = pd.read_csv(\"switchboard_data/switchboard/test_results.tsv\",sep=\"\\t\",header=None)\n",
    "df_results_csv = pd.DataFrame({'Is_Response':df_results.idxmax(axis=1)})\n",
    "\n",
    "classification = []\n",
    "for row in df_results_csv.itertuples():\n",
    "    a,b = row\n",
    "    classification.append(b)\n",
    "print(classification[:5])\n",
    "print(len(classification))\n",
    "\n",
    "precision, recall, fscore, support = score(np.array(ground_truth), classification)\n",
    "print(\"Detailed evaluation:\")\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "sum_p = 0\n",
    "for i, elem in enumerate(fscore):\n",
    "    sum_p += elem * support[i] / sum(support)\n",
    "print(\"weighted f score \", sum_p)\n",
    "print(\"average f score \", statistics.mean(fscore))\n",
    "\n",
    "conf_mat = np.zeros(shape=(54,54))\n",
    "for i, val in enumerate(ground_truth):\n",
    "    conf_mat[val][classification[i]] += 1\n",
    "print(\"conf mat\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "[3, 3, 2, 15, 4]\n",
      "21825\n",
      "[22, 1, 0, 15, 1]\n",
      "21825\n",
      "Detailed evaluation:\n",
      "precision: [0.         0.00012979 0.05479452 0.04237288 0.0816     0.\n",
      " 0.00120773 0.01074787 0.18181818 0.         0.         0.\n",
      " 0.         0.00059488 0.         0.91469194 0.05333333 0.33333333\n",
      " 0.         0.00766284 0.         0.16666667 0.00921659 0.\n",
      " 0.         0.         0.         0.         0.08108108 0.\n",
      " 0.         0.         0.         0.00552486 0.         0.\n",
      " 0.         0.         0.33333333 0.         0.         0.80769231\n",
      " 0.         0.8        0.33333333 0.         1.         0.\n",
      " 0.         0.        ]\n",
      "recall: [0.         0.00483092 0.00699301 0.0027248  0.00695676 0.\n",
      " 0.01769912 0.02166065 0.00262812 0.         0.         0.\n",
      " 0.         0.00980392 0.         1.         0.00254777 0.00692841\n",
      " 0.         0.03389831 0.         0.01785714 0.00626959 0.\n",
      " 0.         0.         0.         0.         0.6        0.\n",
      " 0.         0.         0.         0.11111111 0.         0.\n",
      " 0.         0.         0.2972973  0.         0.         0.875\n",
      " 0.         0.8        0.04       0.         1.         0.\n",
      " 0.         0.        ]\n",
      "fscore: [0.         0.00025278 0.0124031  0.00512033 0.01282051 0.\n",
      " 0.00226116 0.01436696 0.00518135 0.         0.         0.\n",
      " 0.         0.0011217  0.         0.95544554 0.00486322 0.01357466\n",
      " 0.         0.0125     0.         0.03225806 0.00746269 0.\n",
      " 0.         0.         0.         0.         0.14285714 0.\n",
      " 0.         0.         0.         0.01052632 0.         0.\n",
      " 0.         0.         0.31428571 0.         0.         0.84\n",
      " 0.         0.8        0.07142857 0.         1.         0.\n",
      " 0.         0.        ]\n",
      "support: [ 112  207  572 1835 7331   85  113 1108 3805 2506  133  139   75  102\n",
      "  285  193 1570  433  112   59   12   56  319   25    8   65   55   22\n",
      "    5   12   32    2    2    9  256    3   34   10   37    5   11   24\n",
      "    3   10   25    1    3    2    1    1]\n",
      "weighted f score  0.0180873013565828\n",
      "average f score  0.0851745963408505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, statistics\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "ground_truth = []\n",
    "with open('switchboard_data/test_switch_backup.tsv') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    for i,row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        ground_truth.append(int(row[2]))\n",
    "        \n",
    "print(\"ground truth\")\n",
    "print(ground_truth[:5])\n",
    "print(len(ground_truth))\n",
    "\n",
    "\n",
    "df_results = pd.read_csv(\"switchboard_data/switchboard/test_results_30.tsv\",sep=\"\\t\",header=None)\n",
    "df_results_csv = pd.DataFrame({'Is_Response':df_results.idxmax(axis=1)})\n",
    "\n",
    "classification = []\n",
    "for row in df_results_csv.itertuples():\n",
    "    a,b = row\n",
    "    classification.append(b)\n",
    "print(classification[:5])\n",
    "print(len(classification))\n",
    "\n",
    "precision, recall, fscore, support = score(np.array(ground_truth), classification)\n",
    "print(\"Detailed evaluation:\")\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "sum_p = 0\n",
    "for i, elem in enumerate(fscore):\n",
    "    sum_p += elem * support[i] / sum(support)\n",
    "print(\"average f score \", statistics.mean(fscore))\n",
    "print(\"weighted f score \", sum_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
